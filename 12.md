this is my code """def update_data_to_identities(self, df, chunk_size=10000):
    """
    Update data in the Glide identities table.
    """
    def get_table_columns(cursor):
        """
        Fetch the column names from the 'glide.glideidentities' table.
        """
        cursor.execute("DESCRIBE glide.glideidentities;")
        return [column[0] for column in cursor.fetchall()]


    def prepare_data_for_update(df, table_columns):
        """
        Prepares the DataFrame by renaming columns and selecting only required ones.
        """
        df_new = df.rename(columns={"EmployeeID": "resourceid"})
        req_columns = [col for col in df_new.columns if col in table_columns]
        set_clause = ", ".join([f"{col} = %s" for col in req_columns])
        return df_new, req_columns, set_clause


    def get_glide_resource_ids(cursor, resourceid_ids):
        """
        Get existing resourceid values from the database.
        """
        format_strings = ", ".join(["%s"] * len(resourceid_ids))
        query = f"SELECT resourceid FROM glide.glideidentities WHERE resourceid IN ({format_strings})"
        cursor.execute(query, tuple(resourceid_ids))
        return set(row[0] for row in cursor.fetchall())


    def log_missing_resource_ids(missing_resource_ids):
        """
        Logs the missing resource IDs in batches.
        """
        if missing_resource_ids:
            logging.info(f"Missing resourceid values in glide.glideidentities: {missing_resource_ids}")


    def update_data_batch(cursor, update_query, data_to_update):
        """
        Executes the update query for a batch of data.
        """
        try:
            cursor.executemany(update_query, data_to_update)
        except Exception as e:
            logging.error(f"Error processing batch: {str(e)}")
            cursor.connection.rollback()
            raise

    try:
        cursor = self.glide_connection.cursor()

        # Fetch table columns
        table_columns = get_table_columns(cursor)

        # Prepare DataFrame for update
        df_new, req_columns, set_clause = prepare_data_for_update(df, table_columns)

        update_query_template = f"UPDATE glide.glideidentities SET {set_clause} WHERE resourceid = %s"

        # Fetch all existing resource IDs in one go
        graphdb_resource_ids = df_new['resourceid'].unique().tolist()
        glide_resource_ids = get_glide_resource_ids(cursor, graphdb_resource_ids)

        # Log all missing resource IDs at once
        missing_resource_ids = [resource_id for resource_id in graphdb_resource_ids if resource_id not in glide_resource_ids]
        logging.info(f"Count of missing resource_ids: {len(missing_resource_ids)}")
        log_missing_resource_ids(missing_resource_ids)

        # Filter out rows with missing resource IDs
        df_new = df_new[df_new['resourceid'].isin(glide_resource_ids)]
        logging.info(f"Count of df_new: {df_new.shape[0]}")

        # Precompute column indices
        req_columns_idx = [df_new.columns.get_loc(col) for col in req_columns]
        resourceid_idx = df_new.columns.get_loc('resourceid')

        # Insert in chunks using executemany
        for start in range(0, len(df_new), chunk_size):
            end = start + chunk_size
            batch = df_new.iloc[start:end]

            # Prepare data for update
            batch_values = batch.values
            data_to_update = [
                tuple([row[idx] for idx in req_columns_idx] + [row[resourceid_idx]])
                for row in batch_values
            ]

            # Execute batch update
            update_data_batch(cursor, update_query_template, data_to_update)

        self.glide_connection.commit()
        cursor.close()
        logging.info(f"Data updated in Glide identities table successfully for {len(df_new)} records.")

    except Exception as e:
        logging.error(f"Error fetching data: {str(e)} - Debug <Class: IAM DataExport | Method: update_data_to_identities()>")
        if self.glide_connection:
            self.glide_connection.rollback()
        raise""" use this logic """def update_data_to_identities(self, df: pd.DataFrame, chunk_size: int = 10000):
    """
    Update data in Glide identities table
    """
    try:
        cursor = self.glide_connection.cursor()

        # Fetch all employeeids from the input DataFrame
        df_employee_ids = df['employeeid'].unique().tolist()

        # Fetch existing data from the Glide database for the given employeeids
        existing_data_query = """
            SELECT employeeid, L1Managerid, L2Managerid, L3Managerid, L4Managerid, L5Managerid, 
                   L6Managerid, L7Managerid, L8Managerid, L9Managerid, L10Managerid, 
                   L11Managerid, L12Managerid, L13Managerid, L14Managerid, L15Managerid
            FROM glide.glideidentities
            WHERE employeeid IN (%s)
        """ % ','.join(['%s'] * len(df_employee_ids))
        
        cursor.execute(existing_data_query, df_employee_ids)
        existing_data = cursor.fetchall()

        # Convert existing data to a DataFrame
        existing_df = pd.DataFrame(existing_data, columns=[
            'employeeid', 'L1Managerid', 'L2Managerid', 'L3Managerid', 'L4Managerid', 'L5Managerid', 
            'L6Managerid', 'L7Managerid', 'L8Managerid', 'L9Managerid', 'L10Managerid', 
            'L11Managerid', 'L12Managerid', 'L13Managerid', 'L14Managerid', 'L15Managerid'
        ])

        # Merge the existing data with the new data on employeeid
        merged_df = df.merge(existing_df, on='employeeid', suffixes=('_new', '_old'))

        # Identify rows where any of the manager IDs have changed
        change_columns = [f'L{i}Managerid' for i in range(1, 16)]
        change_mask = merged_df[[f'{col}_new' != f'{col}_old' for col in change_columns]].any(axis=1)
        changed_df = merged_df[change_mask]

        # Prepare the SET clause for the UPDATE statement
        set_clause = ', '.join([f"{col} = %s" for col in change_columns])

        # Prepare the UPDATE query template
        update_query_template = f"""
            UPDATE glide.glideidentities 
            SET {set_clause} 
            WHERE employeeid = %s
        """

        # Prepare data for update
        data_to_update = [
            tuple([row[f'{col}_new'] for col in change_columns] + [row['employeeid']]) 
            for _, row in changed_df.iterrows()
        ]

        # Update in chunks
        for i in range(0, len(data_to_update), chunk_size):
            batch = data_to_update[i:i + chunk_size]
            cursor.executemany(update_query_template, batch)
            self.glide_connection.commit()

        cursor.close()
        logging.info(f"Data updated in Glide identities table successfully for {len(changed_df)} records.")
    
    except Exception as e:
        logging.error(f"Error updating data: {str(e)} - Debug <Class: IAM DataExport | Method: update_data_to_identities()>")
        if self.glide_connection:
            self.glide_connection.rollback()
        raise

### Explanation:

1. **Fetch Existing Data**: The `existing_data_query` fetches the current manager IDs for the `employeeid`s present in the DataFrame.

2. **Merge DataFrames**: The `merged_df` DataFrame is created by merging the existing data from MySQL with the new data from the DataFrame on `employeeid`.

3. **Identify Changes**: The `change_mask` identifies rows where any of the manager IDs have changed.

4. **Prepare Update Statements**: The `set_clause` is dynamically generated based on the columns that need to be updated. The `update_query_template` is then created using this clause.

5. **Execute Updates in Batches**: The updates are executed in batches using `executemany` to improve performance and avoid overwhelming the database.

### Notes:
- **Error Handling**: The function includes error handling to log errors and rollback transactions if something goes wrong.
- **Logging**: Logging is used to track the progress and success of the updates.
- **Performance**: The function processes updates in chunks to manage memory usage and improve performance.

This approach ensures that only the rows with changes