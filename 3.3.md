Got it! If you're certain that `entitle_name` and `entitle_source` are the **primary keys** in the `entitlement_master` table and thus **unique**, you **can omit `ORDER BY`** entirely.

In that case, the SQL query can simply be:

```sql
SELECT entitle_name, entitle_source
FROM entitlement_master
LIMIT {batch_size} OFFSET {offset};
```

This will still provide unique records because the **primary key** guarantees that no duplicates will occur, even without `ORDER BY`.

### ‚úÖ Why Removing `ORDER BY` Works in This Case:

1. **Primary Key Uniqueness**: Since `(entitle_name, entitle_source)` is the **primary key**, each combination is unique. So there‚Äôs no risk of duplicates or skipping records between batches.
2. **Performance**: Without `ORDER BY`, the query won't have to **sort** the entire table, making it **much faster**, especially for large datasets.
3. **Consistency**: You still get non-overlapping batches as long as the `offset` is incremented correctly (e.g., `OFFSET 0, 10000`, `OFFSET 10000, 20000`, etc.).

### üìù Updated Query and Python Logic Without `ORDER BY`:

Here‚Äôs the full Python logic that skips the `ORDER BY` and uses just `LIMIT` and `OFFSET`:

```python
# Setup
batch_size = 50000  # Adjust based on memory
offset = 0
final_result = []

while True:
    # Step 1: Fetch a batch of rows (no ORDER BY)
    query_em = f"""
        SELECT entitle_name, entitle_source
        FROM entitlement_master
        LIMIT {batch_size} OFFSET {offset};
    """
    em_batch = pd.read_sql(query_em, engine)

    if em_batch.empty:
        break  # Exit when no more rows are returned

    # Step 2: Fetch related eservice_data for this batch
    keys = tuple(em_batch.itertuples(index=False, name=None))
    if not keys:
        offset += batch_size
        continue

    query_ed = f"""
        SELECT entitle_name, platform AS entitle_source, rank, owner
        FROM eservice_data
        WHERE (entitle_name, platform) IN %s
    """
    ed_df = pd.read_sql(query_ed, engine, params=(keys,))

    # Step 3: Merge and pivot
    merged = em_batch.merge(ed_df, on=['entitle_name', 'entitle_source'], how='left')
    pivoted = merged.pivot_table(
        index=['entitle_name', 'entitle_source'],
        columns='rank',
        values='owner',
        aggfunc='first'
    ).rename(columns={1: 'owner1', 2: 'owner2', 3: 'owner3'}).reset_index()

    final_result.append(pivoted)

    # Update offset for the next batch
    offset += batch_size

# Combine all batches
result_df = pd.concat(final_result, ignore_index=True)
# Save to database or CSV
result_df.to_sql('entitlement_owners', engine, if_exists='replace', index=False)
```

### üìà **Advantages**:

* **No sorting overhead**: You save time by **not sorting** the entire table.
* **Efficient**: Direct `LIMIT` + `OFFSET` batching ensures you handle manageable chunks of data.
* **Scalable**: This will scale for large datasets (like your 8.8 million rows).

### üîß Notes:

* The query is **faster** without the `ORDER BY`, but **you still need to increment `OFFSET`** to ensure the batches are processed in sequence.
* Since the keys are unique, there‚Äôs no risk of duplicate rows between batches.

Would you like further guidance on how to handle specific edge cases or improve performance further?
