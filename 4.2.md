Yes, you need **only minimal changes** in `main.py` to support the retry mechanism added in `data_ingestion.py`. Since the retry logic is **encapsulated inside the ingestion class**, you donâ€™t need to manage retries from `main.py`.

However, if you want to make it more **explicit and configurable**, here are **optional changes**:

---

### âœ… 1. **Add `max_retries` to `config.yaml`** (optional but clean)

If you want to configure retry attempts from a config file:

```yaml
max_retries: 3
```

Then in `main.py`, you can read it:

```python
max_retries = configstream.get("max_retries", 3)
```

---

### âœ… 2. **Pass `max_retries` to your ingestion call**

Update the method call inside `create_graph_for_entitlements()` like this:

```python
ent_raw = iam_ingestion.fetch_Batchdata_from_entitlement_master(
    feature_name, sql_entitlements, chunksize, offset, max_retries=3
)
```

Or dynamically from config:

```python
ent_raw = iam_ingestion.fetch_Batchdata_from_entitlement_master(
    feature_name, sql_entitlements, chunksize, offset, max_retries=configstream.get("max_retries", 3)
)
```

You can do the same in `fetch_batched_data_for_feature_template()` if you want to apply retry there too.

---

### âœ… 3. **(Optional) Log retry configuration**

Add a log at the beginning of `main.py` for clarity:

```python
logging.info(f"Using max_retries = {max_retries} for DB batch operations")
```

---

### ðŸ”š Summary

If you're happy with the retry logic being **self-contained in `IAMDataIngestion`**, no change is *required* in `main.py`. But for best practice, **making `max_retries` configurable** is a nice improvement.

Would you like a snippet to apply retry logic in `fetch_batched_data_for_feature_template()` as well?
