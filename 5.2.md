You're right! I should have incorporated that logic as part of the refactor. Let me explain why I initially left it out and how it can be properly integrated into the new structure.

The block you referenced handles the backup and deletion of old data, which is an important part of the report generation workflow. We should indeed keep that logic, but it should be moved to a separate function to ensure modularity and better separation of concerns.

I'll add the backup logic in a helper function, then integrate it into the main report generation workflow.

Hereâ€™s how the code can be updated:

### Updated Refactored Code

```python
import pandas as pd
import argparse
import datetime
import logging
import os
import tracemalloc
import csv

from src.db_operation.ne4j_operations import IAM_GraphOperations
from src.db_operation.glide_operations import IAM_DataExport

# Constants
MAX_MANAGER_LEVEL = 5

# Helper Functions
def load_configuration(config_file):
    """Loads the configuration data from the specified file."""
    base_config_stream = get_config_stream(config_file)
    database_configs = read_creds(config_file)
    config_stream = base_config_stream["supervisor_hierarchy_report_ProjConfig"]
    
    return base_config_stream, database_configs, config_stream

def create_report_filename(report_directory, report_file_name, report_file_type):
    """Generates the report filename with a timestamp."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    return os.path.join(report_directory, f"{report_file_name}_{timestamp}.{report_file_type}")

def run_query(query, iam_graph_operations):
    """Executes a Cypher query and returns the results as a DataFrame."""
    try:
        with iam_graph_operations.driver.session() as session:
            result = session.run(query)
        return pd.DataFrame([record.values() for record in result], columns=result.keys())
    except Exception as e:
        logging.error(f"Error running query: {e}")
        return pd.DataFrame()  # Return an empty DataFrame in case of error

def expand_manager_levels(row, manager_level):
    """Expands manager levels into multiple columns."""
    levels = row['ManagerLevel']
    max_levels = manager_level
    expanded = levels + [None] * (max_levels - len(levels))
    return pd.Series(expanded, index=[f'L{i+1}ManagerID' for i in range(max_levels)])

def handle_backup_and_deletion(iam_data_export_glide, chunk_size):
    """Handles the backup and deletion of old data."""
    try:
        backup_success = iam_data_export_glide.backup_table_data(
            table_name="glide.glidesupervisorhierarchy",
            backup_table_name="glide.glidesupervisorhierarchy_backup"
        )

        if backup_success:
            iam_data_export_glide.del_7days_old_data(
                backup_table_name="glide.glidesupervisorhierarchy_backup",
                chunk_size=chunk_size
            )
            iam_data_export_glide.del_data_frm_table(
                table_name="glide.glidesupervisorhierarchy",
                chunk_size=chunk_size
            )
            return True
        else:
            logging.info("Backup failed, skipping deletion of old data")
            return False
    except Exception as e:
        logging.error(f"Error in backup or deletion process: {e}")
        return False

def generate_supervisor_report(iam_graph_operations, iam_data_export_glide, batch_size, chunk_size, offset, manager_level, report_file):
    """Generates the supervisor hierarchy report."""
    flag_success = False
    batch_counter = 1

    # Handle backup and deletion of old data
    if not handle_backup_and_deletion(iam_data_export_glide, chunk_size):
        logging.error("Backup and deletion failed, aborting report generation.")
        return False

    with open(report_file, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        header_written = False

        while True:
            try:
                # Step 1: Fetch manager hierarchy
                fetch_manager_hierarchy_query_formatted = fetch_manager_hierarchy_query.format(batch_size=batch_size, offset=offset)
                manager_ids_df = run_query(fetch_manager_hierarchy_query_formatted, iam_graph_operations)

                # Exit loop if no more records are found
                if not len(manager_ids_df):
                    break

                # Extract EmployeeID column for rows where Manager Level == 0
                manager_ids_df_manager_level_0 = manager_ids_df[manager_ids_df['ManagerLevel'] == 0]
                if not manager_ids_df_manager_level_0.empty:
                    logging.info(f"EmployeeIDs with ManagerLevel - 0: {manager_ids_df_manager_level_0['EmployeeID'].tolist()}")

                manager_ids_df = manager_ids_df[manager_ids_df['ManagerLevel'] != 0]
                filtered_df = manager_ids_df[manager_ids_df['ManagerLevel'] > manager_level]
                
                if not filtered_df.empty:
                    logging.info(f"Employee IDs with ManagerLevel > {manager_level}: {filtered_df['EmployeeID'].tolist()}")

                # Step 2: Query manager details
                manager_ids = manager_ids_df['ManagerLevel'].explode().unique()
                manager_ids_list = manager_ids.tolist()
                managers_data_from_graph = iam_graph_operations.fetch_users_data_from_graph(manager_ids_list)
                
                missing_manager_ids = set(manager_ids_list) - set(managers_data_from_graph['ManagerID'])
                if missing_manager_ids:
                    logging.info(f"Missing manager IDs in graph DB: {missing_manager_ids}")

                # Step 3: Merge manager details
                manager_dict = managers_data_from_graph.set_index('ManagerID').to_dict('index')
                expanded_df = manager_ids_df.apply(expand_manager_levels, axis=1, manager_level=manager_level)
                result_df = pd.concat([manager_ids_df[['EmployeeID', 'ManagerLevel']], expanded_df], axis=1)

                for i in range(1, manager_level + 1):
                    level_id = f'L{i}ManagerID'
                    result_df[f'L{i}ManagerFirstName'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerFirstName'))
                    result_df[f'L{i}ManagerLastName'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerLastName'))
                    result_df[f'L{i}ManagerEmailAddress'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerEmailAddress'))
                    result_df[f'L{i}ManagerJobCodeDescription'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerJobCodeDescription'))
                    result_df[f'L{i}ManagerTelephoneNumber'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerTelephoneNumber'))
                
                column_order = ['EmployeeID', 'ManagerLevel']
                for i in range(1, manager_level + 1):
                    column_order.extend([f'L{i}ManagerID', f'L{i}ManagerFirstName', f'L{i}ManagerLastName',
                                         f'L{i}ManagerEmailAddress', f'L{i}ManagerJobCodeDescription', f'L{i}ManagerTelephoneNumber'])
                
                result_df = result_df[column_order]
                iam_data_export_glide.write_data_to_supervisorhierarchy(result_df, chunk_size)

                offset += batch_size
                batch_counter += 1
            except Exception as e:
                logging.error(f"Error in batch {batch_counter} at offset {offset}: {e}")
                break

        return True  # If all batches ran successfully

def main():
    tracemalloc.start()
    logging.basicConfig(level=logging.INFO)
    
    # Parse arguments
    parser = argparse.ArgumentParser(description="Generate supervisor hierarchy report")
    parser.add_argument("log_location", type=str, help="Specify the log location")
    args = parser.parse_args()

    configuration_file = "/config/config.yaml"
    base_config_stream, database_configs, config_stream = load_configuration(configuration_file)

    # Extract config values
    report_directory = config_stream["report_directory"]
    report_file_name = config_stream["report_file_name"]
    report_file_type = config_stream["report_file_type"]
    batch_size = config_stream["batch_size"]
    chunk_size = config_stream["chunk_size"]
    offset = config_stream["offset"]
    fetch_manager_hierarchy_query = base_config_stream["fetch_manager_hierarchy_query"]

    if not report_directory:
        raise ValueError("Report output file location is not configured")

    report_output_file = create_report_filename(report_directory, report_file_name, report_file_type)
    
    # Initialize database operations
    iam_graph_operations = IAM_GraphOperations(database_configs)
    iam_data_export_glide = IAM_DataExport(database_configs)

    # Generate the supervisor report
    if generate_supervisor_report(iam_graph_operations, iam_data_export_glide, batch_size, chunk_size, offset, MAX_MANAGER_LEVEL, report_output_file):
        logging.info(f"Supervisor hierarchy report job ran successfully with batch_size {batch_size}")
    else:
        logging.error("Supervisor hierarchy report creation job failed, please debug logs")

    tracemalloc.stop()

if __name__ == "__main__":
    main()
```

### Key Changes:
1. **Backup and Deletion Logic**: Moved the backup and deletion logic into the `handle_backup_and_deletion` function for modularity.
   - This function is now invoked before starting the report generation in the `generate_supervisor_report` function.
   - If the backup or deletion fails, the process will abort the report generation, as indicated by the `False` return value.
   
2. **CSV File Handling**: Kept the `with open(report_file, "w", newline='') as csvfile` block in the `generate_supervisor_report` function. This will now open the CSV file and handle writing the data inside the loop where report generation occurs.

3. **Error Handling**: Both backup and data export functions now include error handling to ensure that issues are logged if any part of the process fails.

This updated

 code structure keeps everything modular, ensures that the backup and cleanup are done before the main report generation, and provides more detailed error handling.