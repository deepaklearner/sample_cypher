Sure! Here's a **complete Python script** using `pandas` and `SQLAlchemy` that:

1. Reads data from the `entitlement_master` table in **offset-based batches** (no `ORDER BY`).
2. Joins each batch with the `eservice_data` table on `(entitle_name, entitle_source = platform)`.
3. Pivots the results to get `owner1`, `owner2`, `owner3` based on `rank`.
4. Appends each processed batch to a final result.
5. Saves the final combined data to a **new table** or **CSV file**.

---

### ‚úÖ Assumptions:

* You‚Äôre using **PostgreSQL** (adjust connection string if using another DB).
* The schema includes:

  * `entitlement_master(entitle_name, entitle_source, dat1, dat2, dat3, dat4, dat5)`
  * `eservice_data(entitle_name, platform, rank, owner)`

---

### üßæ Full Code:

```python
import pandas as pd
from sqlalchemy import create_engine

# --- üîß DB Setup ---
# Update with your actual DB credentials
engine = create_engine("postgresql+psycopg2://user:password@host:port/database")

# --- ‚öôÔ∏è Batching Config ---
batch_size = 50000
offset = 0
final_result = []

while True:
    print(f"Processing batch OFFSET {offset}...")

    # Step 1: Read a batch from entitlement_master (no ORDER BY)
    query_em = f"""
        SELECT entitle_name, entitle_source, dat1, dat2, dat3, dat4, dat5
        FROM entitlement_master
        LIMIT {batch_size} OFFSET {offset};
    """
    em_batch = pd.read_sql(query_em, engine)

    if em_batch.empty:
        print("No more rows to process.")
        break  # Done with all data

    # Step 2: Get unique (entitle_name, entitle_source) pairs for join
    keys = tuple(em_batch[['entitle_name', 'entitle_source']].drop_duplicates().itertuples(index=False, name=None))
    if not keys:
        offset += batch_size
        continue

    # Step 3: Fetch related eservice_data
    placeholders = ', '.join(['%s'] * len(keys))
    query_ed = f"""
        SELECT entitle_name, platform AS entitle_source, rank, owner
        FROM eservice_data
        WHERE (entitle_name, platform) IN ({placeholders});
    """
    ed_df = pd.read_sql(query_ed, engine, params=keys)

    # Step 4: Merge and pivot on rank
    merged = em_batch.merge(ed_df, on=['entitle_name', 'entitle_source'], how='left')

    pivoted = merged.pivot_table(
        index=['entitle_name', 'entitle_source', 'dat1', 'dat2', 'dat3', 'dat4', 'dat5'],
        columns='rank',
        values='owner',
        aggfunc='first'
    ).rename(columns={1: 'owner1', 2: 'owner2', 3: 'owner3'}).reset_index()

    final_result.append(pivoted)
    offset += batch_size

# --- üì¶ Final Combine ---
result_df = pd.concat(final_result, ignore_index=True)

# --- üíæ Save the result (Choose one) ---

# Option 1: Save to a new table
result_df.to_sql('entitlement_owners', engine, if_exists='replace', index=False)

# Option 2: Save to CSV
# result_df.to_csv('entitlement_owners.csv', index=False)

print("‚úÖ Processing complete.")
```

---

### üõ†Ô∏è Optional: Create Indexes for Speed (Run once in SQL)

```sql
CREATE INDEX idx_entitlement_keys ON entitlement_master(entitle_name, entitle_source);
CREATE INDEX idx_eservice_keys ON eservice_data(entitle_name, platform, rank);
```

---

This script is ready to run as-is. You can extend it later for error handling, logging, or multiprocessing if needed.

Would you like this adapted for a different DB (like MySQL or SQL Server)?
