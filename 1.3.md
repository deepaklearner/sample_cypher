Great — let's refactor your logic to use **pandas DataFrame operations** as much as possible while minimizing row-by-row processing. However, since checking data in Neo4j requires I/O operations, we **cannot completely avoid some row-wise processing** for those checks. But we can:

* Use batch queries where possible.
* Store results back into a DataFrame for vectorized post-processing.

---

### ✅ Final Goal:

Given a `data_frame` with `employeeNumber` and `conversionType`, apply these rules:

1. **User not found** → Reason: `"User not found"`
2. **User found but lacks label OnLeave or Active** → Reason: `"User node do not contain OnLeave or Active label"`
3. **User is already an Employee** → Reason: `"User is already an Employee"`
4. **User does not have Domestic label** → Reason: `"Domestic label not present"`

---

### ✅ Pandas + Neo4j Hybrid Solution

```python
import pandas as pd
from neo4j import GraphDatabase

# Neo4j connection setup
uri = "bolt://localhost:7687"
user = "neo4j"
password = "your_password"
driver = GraphDatabase.driver(uri, auth=(user, password))

# Your input DataFrame
# data_frame = pd.DataFrame({
#     'employeeNumber': ['123', '456', '789'],
#     'conversionType': ['TypeA', 'TypeB', 'TypeC']
# })

# Function to batch query user labels
def fetch_user_labels(employee_numbers):
    with driver.session() as session:
        query = """
        UNWIND $emp_numbers AS emp_number
        OPTIONAL MATCH (u {employeeNumber: emp_number})
        RETURN emp_number AS employeeNumber, labels(u) AS labels
        """
        result = session.run(query, emp_numbers=employee_numbers)
        return pd.DataFrame([r.data() for r in result])

# Step 1: Query Neo4j once for all employeeNumbers
employee_numbers = data_frame['employeeNumber'].unique().tolist()
user_labels_df = fetch_user_labels(employee_numbers)

# Step 2: Merge results into original DataFrame
merged_df = data_frame.merge(user_labels_df, on='employeeNumber', how='left')

# Step 3: Apply validation rules vectorized
def determine_reason(row):
    labels = row['labels']
    
    if pd.isna(labels):
        return "User not found"
    
    if not any(label in labels for label in ["OnLeave", "Active"]):
        return "User node do not contain OnLeave or Active label"
    
    if "Employee" in labels:
        return "User is already an Employee"
    
    if "Domestic" not in labels:
        return "Domestic label not present"
    
    return None  # Valid user

# Apply rule logic
merged_df['Reason'] = merged_df.apply(determine_reason, axis=1)

# Step 4: Split into valid and skipped
skipped_df = merged_df[merged_df['Reason'].notnull()].copy()
valid_df = merged_df[merged_df['Reason'].isnull()].copy()

# Optional: Drop labels column if not needed
# skipped_df.drop(columns=['labels'], inplace=True)
# valid_df.drop(columns=['labels'], inplace=True)

# Result
print("❌ Skipped:")
print(skipped_df[['employeeNumber', 'conversionType', 'Reason']])

print("\n✅ Valid:")
print(valid_df[['employeeNumber', 'conversionType']])
```

---

### ✅ Advantages of this Approach:

* Only **one query** to Neo4j (batch via `UNWIND`) — efficient and scalable.
* Data validation is handled in pandas, not per-row in Python.
* Easily extendable if more rules are needed.

Would you like me to add a column explaining which rule each user passed or failed?
