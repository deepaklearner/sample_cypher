Absolutely! You can leverage **DataFrame operations** combined with the Neo4j `UNWIND` approach to avoid `iterrows()` loops, making your code cleaner and potentially faster.

---

### Strategy:

1. Use `UNWIND` in Neo4j to fetch all labels for employeeNumbers in bulk.
2. Build a DataFrame from that result: columns like `employeeNumber` and `labels` (as list).
3. Merge this labels DataFrame with your original filtered `employee_df` on `employeeNumber`.
4. Use vectorized Pandas operations to apply your mismatch logic and create a "Reason" column.
5. Split into mismatched and valid DataFrames.

---

### Example implementation:

```python
import pandas as pd
from neo4j import GraphDatabase

# Neo4j Config
uri = "bolt://localhost:7687"
username = "neo4j"
password = "your_password"  # replace with your password

driver = GraphDatabase.driver(uri, auth=(username, password))

employee_df = df[df['userType'] == 'Employee'].copy()

emp_nums = employee_df['employeeNumber'].tolist()

def get_users_labels(tx, emp_nums):
    query = """
    UNWIND $empNums AS empNum
    MATCH (u:User {employeeNumber: empNum})
    RETURN empNum AS employeeNumber, labels(u) AS labels
    """
    result = tx.run(query, empNums=emp_nums)
    return [record.data() for record in result]

with driver.session() as session:
    labels_records = session.read_transaction(get_users_labels, emp_nums)

# Build labels DataFrame
labels_df = pd.DataFrame(labels_records)

# Some users might be missing in Neo4j, add empty lists for missing ones
labels_df.set_index('employeeNumber', inplace=True)
labels_df = labels_df.reindex(employee_df['employeeNumber']).fillna({'labels': [[]]})
labels_df.reset_index(inplace=True)

# Merge labels into employee_df
merged_df = employee_df.merge(labels_df, on='employeeNumber', how='left')

# Define mismatch conditions as vectorized masks
cond_user_not_found = merged_df['labels'].apply(lambda x: len(x) == 0)

cond_contractor_wrong_record = merged_df.apply(
    lambda row: 'Contractor' in row['labels'] and row['recordType'] != 'Conversion', axis=1
)

cond_status_mismatch = merged_df.apply(
    lambda row: (
        row['employmentStatus'] in ['A', 'L'] and
        not any(label in ['Active', 'OnLeave'] for label in row['labels'])
    ), axis=1
)

# Create Reason column with empty strings initially
merged_df['Reason'] = ""

# Assign reasons based on conditions
merged_df.loc[cond_user_not_found, 'Reason'] = "User node not found in Neo4j"
merged_df.loc[cond_contractor_wrong_record, 'Reason'] = "Contractor label present but recordType is not 'Conversion'"
merged_df.loc[cond_status_mismatch, 'Reason'] = (
    "employmentStatus in ['A','L'] but missing 'Active'/'OnLeave' label in Neo4j"
)

# Filter mismatches and valid
mismatch_df = merged_df[merged_df['Reason'] != ''][['employeeNumber', 'Reason']]
valid_df = merged_df[merged_df['Reason'] == '']

# Save mismatch log
mismatch_df.to_csv("mismatch_log.csv", index=False)

print("âœ… Mismatch log saved: mismatch_log.csv")
print(f"Valid records count: {len(valid_df)}")
print(f"Mismatch records count: {len(mismatch_df)}")
```

---

### Benefits of this approach:

* **No explicit Python loops over rows** (except minimal use of `apply` which can be improved with further vectorization if needed).
* Clear separation of mismatch reasons using masks.
* Efficient Neo4j data retrieval with `UNWIND`.
* Easy to maintain and extend.

---

If your DataFrame is very large and performance is critical, we can further optimize those `.apply()` calls or use other approaches like `explode` or specialized libraries.

Want me to help optimize it further?
