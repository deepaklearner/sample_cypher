Here is the full refactored code based on the suggestions for modularization, error handling, logging, and maintainability:

```python
import pandas as pd
import argparse
import datetime
import logging
import os
import tracemalloc
import csv

from src.db_operation.ne4j_operations import IAM_GraphOperations
from src.db_operation.glide_operations import IAM_DataExport

# Constants
MAX_MANAGER_LEVEL = 5

# Helper Functions
def load_configuration(config_file):
    """Loads the configuration data from the specified file."""
    base_config_stream = get_config_stream(config_file)
    database_configs = read_creds(config_file)
    config_stream = base_config_stream["supervisor_hierarchy_report_ProjConfig"]
    
    return base_config_stream, database_configs, config_stream

def create_report_filename(report_directory, report_file_name, report_file_type):
    """Generates the report filename with a timestamp."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    return os.path.join(report_directory, f"{report_file_name}_{timestamp}.{report_file_type}")

def run_query(query, iam_graph_operations):
    """Executes a Cypher query and returns the results as a DataFrame."""
    try:
        with iam_graph_operations.driver.session() as session:
            result = session.run(query)
        return pd.DataFrame([record.values() for record in result], columns=result.keys())
    except Exception as e:
        logging.error(f"Error running query: {e}")
        return pd.DataFrame()  # Return an empty DataFrame in case of error

def expand_manager_levels(row, manager_level):
    """Expands manager levels into multiple columns."""
    levels = row['ManagerLevel']
    max_levels = manager_level
    expanded = levels + [None] * (max_levels - len(levels))
    return pd.Series(expanded, index=[f'L{i+1}ManagerID' for i in range(max_levels)])

def generate_supervisor_report(iam_graph_operations, iam_data_export_glide, batch_size, chunk_size, offset, manager_level, report_file):
    """Generates the supervisor hierarchy report."""
    flag_success = False
    batch_counter = 1

    while flag_success:
        try:
            # Step 1: Fetch manager hierarchy
            fetch_manager_hierarchy_query_formatted = fetch_manager_hierarchy_query.format(batch_size=batch_size, offset=offset)
            manager_ids_df = run_query(fetch_manager_hierarchy_query_formatted, iam_graph_operations)

            # Exit loop if no more records are found
            if not len(manager_ids_df):
                break

            # Extract EmployeeID column for rows where Manager Level == 0
            manager_ids_df_manager_level_0 = manager_ids_df[manager_ids_df['ManagerLevel'] == 0]
            if not manager_ids_df_manager_level_0.empty:
                logging.info(f"EmployeeIDs with ManagerLevel - 0: {manager_ids_df_manager_level_0['EmployeeID'].tolist()}")

            manager_ids_df = manager_ids_df[manager_ids_df['ManagerLevel'] != 0]
            filtered_df = manager_ids_df[manager_ids_df['ManagerLevel'] > manager_level]
            
            if not filtered_df.empty:
                logging.info(f"Employee IDs with ManagerLevel > {manager_level}: {filtered_df['EmployeeID'].tolist()}")

            # Step 2: Query manager details
            manager_ids = manager_ids_df['ManagerLevel'].explode().unique()
            manager_ids_list = manager_ids.tolist()
            managers_data_from_graph = iam_graph_operations.fetch_users_data_from_graph(manager_ids_list)
            
            missing_manager_ids = set(manager_ids_list) - set(managers_data_from_graph['ManagerID'])
            if missing_manager_ids:
                logging.info(f"Missing manager IDs in graph DB: {missing_manager_ids}")

            # Step 3: Merge manager details
            manager_dict = managers_data_from_graph.set_index('ManagerID').to_dict('index')
            expanded_df = manager_ids_df.apply(expand_manager_levels, axis=1, manager_level=manager_level)
            result_df = pd.concat([manager_ids_df[['EmployeeID', 'ManagerLevel']], expanded_df], axis=1)

            for i in range(1, manager_level + 1):
                level_id = f'L{i}ManagerID'
                result_df[f'L{i}ManagerFirstName'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerFirstName'))
                result_df[f'L{i}ManagerLastName'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerLastName'))
                result_df[f'L{i}ManagerEmailAddress'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerEmailAddress'))
                result_df[f'L{i}ManagerJobCodeDescription'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerJobCodeDescription'))
                result_df[f'L{i}ManagerTelephoneNumber'] = result_df[level_id].map(lambda x: manager_dict.get(x, {}).get('ManagerTelephoneNumber'))
            
            column_order = ['EmployeeID', 'ManagerLevel']
            for i in range(1, manager_level + 1):
                column_order.extend([f'L{i}ManagerID', f'L{i}ManagerFirstName', f'L{i}ManagerLastName',
                                     f'L{i}ManagerEmailAddress', f'L{i}ManagerJobCodeDescription', f'L{i}ManagerTelephoneNumber'])
            
            result_df = result_df[column_order]
            iam_data_export_glide.write_data_to_supervisorhierarchy(result_df, chunk_size)

            offset += batch_size
            batch_counter += 1
        except Exception as e:
            logging.error(f"Error in batch {batch_counter} at offset {offset}: {e}")
            flag_success = False
            break

    return flag_success

def main():
    tracemalloc.start()
    logging.basicConfig(level=logging.INFO)
    
    # Parse arguments
    parser = argparse.ArgumentParser(description="Generate supervisor hierarchy report")
    parser.add_argument("log_location", type=str, help="Specify the log location")
    args = parser.parse_args()

    configuration_file = "/config/config.yaml"
    base_config_stream, database_configs, config_stream = load_configuration(configuration_file)

    # Extract config values
    report_directory = config_stream["report_directory"]
    report_file_name = config_stream["report_file_name"]
    report_file_type = config_stream["report_file_type"]
    batch_size = config_stream["batch_size"]
    chunk_size = config_stream["chunk_size"]
    offset = config_stream["offset"]
    fetch_manager_hierarchy_query = base_config_stream["fetch_manager_hierarchy_query"]

    if not report_directory:
        raise ValueError("Report output file location is not configured")

    report_output_file = create_report_filename(report_directory, report_file_name, report_file_type)
    
    # Initialize database operations
    iam_graph_operations = IAM_GraphOperations(database_configs)
    iam_data_export_glide = IAM_DataExport(database_configs)

    # Open report file and start report generation
    with open(report_output_file, "w", newline='') as csvfile:
        if generate_supervisor_report(iam_graph_operations, iam_data_export_glide, batch_size, chunk_size, offset, MAX_MANAGER_LEVEL, csvfile):
            logging.info(f"Supervisor hierarchy report job ran successfully with batch_size {batch_size}")
        else:
            logging.error("Supervisor hierarchy report creation job failed, please debug logs")

    tracemalloc.stop()

if __name__ == "__main__":
    main()
```

### Key Refactorings:
1. **Modularized functions**: Code is broken into smaller, more manageable functions, such as `load_configuration`, `create_report_filename`, `run_query`, `expand_manager_levels`, and `generate_supervisor_report`.
   
2. **Main function**: The `main()` function coordinates the process, improving clarity and organization.

3. **Error handling**: Wrapped key logic in try-except blocks to handle errors more gracefully and log them with proper messages.

4. **Logging**: Improved logging and added consistent log messages for all important operations.

5. **Constants**: Replaced magic numbers with constants like `MAX_MANAGER_LEVEL`.

6. **Configuration extraction**: Used a helper function (`load_configuration`) to read configuration details, making the code more flexible and easy to modify.

By applying these practices, the code is easier to maintain, test, and extend.